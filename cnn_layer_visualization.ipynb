{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "na2TRfDlrVo1"
      },
      "source": [
        "import os\n",
        "import copy\n",
        "import numpy as np\n",
        "from PIL import Image, ImageFilter\n",
        "import matplotlib.cm as mpl_color_map\n",
        "\n",
        "import torch\n",
        "from torch.optim import Adam\n",
        "# from torch.autograd import Variable\n",
        "from torchvision import models\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiEe4igh0XfP"
      },
      "source": [
        "def format_np_output(np_arr):\n",
        "    \"\"\"\n",
        "        This is a (kind of) bandaid fix to streamline saving procedure.\n",
        "        It converts all the outputs to the same format which is 3xWxH\n",
        "        with using sucecssive if clauses.\n",
        "    Args:\n",
        "        im_as_arr (Numpy array): Matrix of shape 1xWxH or WxH or 3xWxH\n",
        "    \"\"\"\n",
        "    # Phase/Case 1: The np arr only has 2 dimensions\n",
        "    # Result: Add a dimension at the beginning\n",
        "    if len(np_arr.shape) == 2:\n",
        "        np_arr = np.expand_dims(np_arr, axis=0)\n",
        "    # Phase/Case 2: Np arr has only 1 channel (assuming first dim is channel)\n",
        "    # Result: Repeat first channel and convert 1xWxH to 3xWxH\n",
        "    if np_arr.shape[0] == 1:\n",
        "        np_arr = np.repeat(np_arr, 3, axis=0)\n",
        "    # Phase/Case 3: Np arr is of shape 3xWxH\n",
        "    # Result: Convert it to WxHx3 in order to make it saveable by PIL\n",
        "    if np_arr.shape[0] == 3:\n",
        "        np_arr = np_arr.transpose(1, 2, 0)\n",
        "    # Phase/Case 4: NP arr is normalized between 0-1\n",
        "    # Result: Multiply with 255 and change type to make it saveable by PIL\n",
        "    if np.max(np_arr) <= 1:\n",
        "        np_arr = (np_arr*255).astype(np.uint8)\n",
        "    return np_arr\n",
        "\n",
        "    \n",
        "def save_image(im, path):\n",
        "    \"\"\"\n",
        "        Saves a numpy matrix or PIL image as an image\n",
        "    Args:\n",
        "        im_as_arr (Numpy array): Matrix of shape DxWxH\n",
        "        path (str): Path to the image\n",
        "    \"\"\"\n",
        "    if isinstance(im, (np.ndarray, np.generic)):\n",
        "        im = format_np_output(im)\n",
        "        # print(im.shape)\n",
        "        im = Image.fromarray(im)\n",
        "    im.save(path)\n",
        "\n",
        "\n",
        "def preprocess_image(pil_im, resize_im=True):\n",
        "    \"\"\"\n",
        "        Processes image for CNNs\n",
        "\n",
        "    Args:\n",
        "        PIL_img (PIL_img): PIL Image or numpy array to process\n",
        "        resize_im (bool): Resize to 224 or not\n",
        "    returns:\n",
        "        im_as_var (torch variable): Variable that contains processed float tensor\n",
        "    \"\"\"\n",
        "    # mean and std list for channels (Imagenet)\n",
        "    mean = [0.485, 0.456, 0.406]\n",
        "    std = [0.229, 0.224, 0.225]\n",
        "\n",
        "    #ensure or transform incoming image to PIL image\n",
        "    if type(pil_im) != Image.Image:\n",
        "        try:\n",
        "            pil_im = Image.fromarray(pil_im)\n",
        "        except Exception as e:\n",
        "            print(\"could not transform PIL_img to a PIL Image object. Please check input.\")\n",
        "\n",
        "    # Resize image\n",
        "    if resize_im:\n",
        "        pil_im = pil_im.resize((224, 224), Image.ANTIALIAS)\n",
        "\n",
        "    im_as_arr = np.float32(pil_im)\n",
        "    im_as_arr = im_as_arr.transpose(2, 0, 1)  # Convert array to D,W,H\n",
        "    # Normalize the channels\n",
        "    for channel, _ in enumerate(im_as_arr):\n",
        "        im_as_arr[channel] /= 255\n",
        "        im_as_arr[channel] -= mean[channel]\n",
        "        im_as_arr[channel] /= std[channel]\n",
        "    # Convert to float tensor\n",
        "    im_as_ten = torch.from_numpy(im_as_arr).float()\n",
        "    # Add one more channel to the beginning. Tensor shape = 1,3,224,224\n",
        "    im_as_ten.unsqueeze_(0)\n",
        "    # Convert to Pytorch variable\n",
        "    im_as_ten.requires_grad=True\n",
        "    # im_as_var = Variable(im_as_ten, requires_grad=True)\n",
        "    return im_as_ten\n",
        "\n",
        "\n",
        "def recreate_image(im_as_var):\n",
        "    \"\"\"\n",
        "        Recreates images from a torch variable, sort of reverse preprocessing\n",
        "    Args:\n",
        "        im_as_var (torch variable): Image to recreate\n",
        "    returns:\n",
        "        recreated_im (numpy arr): Recreated image in array\n",
        "    \"\"\"\n",
        "    reverse_mean = [-0.485, -0.456, -0.406]\n",
        "    reverse_std = [1/0.229, 1/0.224, 1/0.225]\n",
        "    recreated_im = copy.copy(im_as_var.data.numpy()[0])\n",
        "    for c in range(3):\n",
        "        recreated_im[c] /= reverse_std[c]\n",
        "        recreated_im[c] -= reverse_mean[c]\n",
        "    recreated_im[recreated_im > 1] = 1\n",
        "    recreated_im[recreated_im < 0] = 0\n",
        "    recreated_im = np.round(recreated_im * 255)\n",
        "\n",
        "    recreated_im = np.uint8(recreated_im).transpose(1, 2, 0)\n",
        "    return recreated_im"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97Agu2NDr3kQ",
        "outputId": "83e810a0-004b-47e8-deea-4e58a7cdce1b"
      },
      "source": [
        "%ls ./"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mdrive\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7dybeAWrJ6W"
      },
      "source": [
        "class CNNLayerVisualizer():\n",
        "    def __init__(self, model, selected_layer, selected_filter):\n",
        "        self.model = model\n",
        "        self.model.cuda()\n",
        "        self.model.eval()\n",
        "        self.selected_layer = selected_layer\n",
        "        self.selected_filter = selected_filter\n",
        "        self.conv_output = 0\n",
        "\n",
        "        if not os.path.exists('./generated'):\n",
        "            os.makedirs('./generated')\n",
        "\n",
        "    def hook_layers(self):\n",
        "        def hook_function(module, grad_in, grad_out):\n",
        "            print(grad_out[0].shape)\n",
        "            self.conv_output = grad_out[0, self.selected_filter]\n",
        "        list(self.model.children())[self.selected_layer][1].register_forward_hook(hook_function)\n",
        "\n",
        "    def visualize_layer(self):\n",
        "        self.hook_layers()\n",
        "        random_image = np.uint8(np.random.uniform(150, 180, (224, 224, 3)))\n",
        "        processed_image = preprocess_image(random_image, False)\n",
        "\n",
        "        optimizer = Adam([processed_image], lr=0.1, weight_decay=1e-6)\n",
        "        for i in range(1, 6):\n",
        "            optimizer.zero_grad()\n",
        "            x = processed_image.cuda()\n",
        "            for index, module in enumerate(self.model.children()):\n",
        "                x = module(x)\n",
        "                if index == self.selected_layer:\n",
        "                    break\n",
        "            loss = -torch.mean(self.conv_output)\n",
        "            print('Iteration:', str(i), 'Loss:', \"{0:.2f}\".format(loss.cpu().data.numpy()))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            self.created_image = recreate_image(processed_image)\n",
        "            if i % 10 == 0:\n",
        "                im_path = './generated/layer_vis_l' + str(self.selected_layer) + \\\n",
        "                '_f' + str(self.selected_filter) + '_iter' + str(i) + '.jpg'\n",
        "                save_image(self.created_image, im_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4NL6q6iygkh"
      },
      "source": [
        "selected_layer = 5\n",
        "selected_filter = 8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Z8iz0gjtz2D"
      },
      "source": [
        "model = models.resnet50(pretrained=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGVxXNDjyXpT",
        "outputId": "74470623-c642-4a11-9636-8e6e317098e4"
      },
      "source": [
        "layer_vis = CNNLayerVisualizer(model, selected_layer, selected_filter)\n",
        "\n",
        "layer_vis.visualize_layer()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([512, 28, 28])\n",
            "Iteration: 1 Loss: -0.03\n",
            "torch.Size([512, 28, 28])\n",
            "Iteration: 2 Loss: -0.09\n",
            "torch.Size([512, 28, 28])\n",
            "Iteration: 3 Loss: -0.14\n",
            "torch.Size([512, 28, 28])\n",
            "Iteration: 4 Loss: -0.19\n",
            "torch.Size([512, 28, 28])\n",
            "Iteration: 5 Loss: -0.22\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7chAhdVuKPA",
        "outputId": "cc881903-7187-4188-c9ec-7aba3a614e4e"
      },
      "source": [
        "model.children()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator object Module.children at 0x7f32e9b58048>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOvwq6hpyVBt",
        "outputId": "ed62a160-4540-4ccf-cfa7-29a9dd2a5385"
      },
      "source": [
        "list(models.resnet18().children())[5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): BasicBlock(\n",
              "    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (downsample): Sequential(\n",
              "      (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (1): BasicBlock(\n",
              "    (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDzGm9TS7yQ5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}